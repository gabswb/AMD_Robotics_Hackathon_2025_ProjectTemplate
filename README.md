# AMD_Robotics_Hackathon_2025_[Project Name]

## Team Information

**Team:** *Stereobot*: Thomas Gaviard, Haoran Wang and Gabriel Schwab

**Summary:** *Two arms, one model to rule them both. Our bi-manual robot picks, scans, and sorts packages autonomously—an end-to-end, modular solution built for accurate and scalable warehouse automation.*

<video src=".assets/IMG_2818.mp4" controls></video>


## Submission Details

### 1. Mission Description
- Our mission demonstrates a real-world warehouse automation use case through a candy warehouse scenario, where a bi-manual robotic system autonomously picks and scans items with one arm and hands them off to a second arm for accurate sorting and packaging, showcasing a modular, end-to-end solution for high-throughput fulfillment.

### 2. Creativity
- Our approach is novel in that we use a single unified model to coordinate a bi-manual robotic task, enabling seamless handoff between picking, scanning, and sorting. The system is fully modular and easy to use, remaining independent of specific scanning or sorting technologies while maintaining high accuracy. By design, it is low-cost and adaptable, allowing the same framework to be deployed across different hardware setups and warehouse workflows without reengineering.

### 3. Technical implementations
- *Teleoperation / Dataset capture*
 <video src=".assets/IMG_2806.mp4" controls></video>
    
- *Training*
We trained ACT on a compact dataset of 150 episodes using one top camera, one scan-state camera, and two arm-mounted cameras, following the LeRobot training recipe for 35K steps on AMD MI300X. To improve robustness, we fine-tuned the model on 30 failure-case episodes.
- *Inference*
AMD Ryzen AI 9 HX370 PC
OS: ubuntu 24.04
ROCm v6.3+
PyTorch v2.7.x
LeRobot: v0.4.1
    - <video src=".assets/IMG_2818.mp4" controls></video>


### 4. Ease of use
- Generalization: The model successfully generalizes to all trained objects and also to new, previously unseen objects.
- Flexibility & Adaptability: The system is fully independent of specific scanning or warehouse setups and operates solely on color feedback for sorting decisions.
- Control Interface: The robot requires minimal inputs—color feedback, an inference script, and items to pick up—making deployment simple and low overhead.

## Additional Links
Video showcase the high accuracy of our solution:
 <video src=".assets/demo.mp4" controls></video>

Model weights on HuggingFace hub: https://huggingface.co/tms-gvd/act-finetune-35k

Datasets: 
- https://huggingface.co/datasets/tms-gvd/scan-v2-merged-01-02-fix0-cycling
- https://huggingface.co/datasets/tms-gvd/scan-v2-cycling-30

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
